{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52929e8",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2025/)    \n",
    "## 1MS041, 2025 \n",
    "&copy;2025 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14707b5e",
   "metadata": {},
   "source": [
    "# Fundamentals of estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd1403",
   "metadata": {},
   "source": [
    "## Example 1, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5de34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y==lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b6960",
   "metadata": {},
   "source": [
    "## Example 2, linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db3ec98",
   "metadata": {},
   "source": [
    "### Understanding Linear Regression as an Estimator\n",
    "\n",
    "Linear regression estimates a **function** that maps inputs to outputs, unlike the mean estimator which estimates a single parameter.\n",
    "\n",
    "**The Setup:**\n",
    "- Given data $(x, y)$ with linear relationship: $y \\approx \\beta_0 + \\beta_1 x + \\epsilon$\n",
    "- Goal: Estimate $g(x) = \\beta_0 + \\beta_1 x$ from the data\n",
    "\n",
    "**The Process:**\n",
    "1. Collect pairs of $(x, y)$ observations\n",
    "2. Choose linear functions: $g(x) = \\beta_0 + \\beta_1 x$\n",
    "3. Find best parameters by minimizing: $\\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2$\n",
    "4. Return estimator $\\hat{g}(x)$ for new predictions\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Empirical Risk Minimization**: Minimize prediction errors on training data\n",
    "- **Function Estimation**: We're estimating an entire function, not just a parameter\n",
    "- **Generalization**: The learned function should work on new, unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(x,y):\n",
    "    # Here x,y is our data\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x,y)\n",
    "    return lambda x1: lr.predict(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d724afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = np.random.uniform(0,1,size=100).reshape(-1,1)\n",
    "sample_y = 3*sample_x.flatten()+np.random.normal(0,1,size=sample_x.shape[0])\n",
    "g_hat = linear_regression(sample_x,sample_y)\n",
    "\n",
    "# Lets plot out function\n",
    "import matplotlib.pyplot as plt\n",
    "x_plot = np.linspace(0,1,10)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,4)\n",
    "plt.plot(x_plot,g_hat(x_plot.reshape(-1,1)))\n",
    "plt.scatter(sample_x,sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a5f54",
   "metadata": {},
   "source": [
    "### Repeat Experiments: $\\hat{g}$ is a Random Function\n",
    "\n",
    "The following cells repeat the estimation experiment multiple times with different datasets. This demonstrates that **$\\hat{g}(x)$ is a random function** - each time we collect new data, we get a different estimated function. The variability shows the uncertainty in our estimates due to sampling randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60beb081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(10):\n",
    "    sample_x = np.random.uniform(0,1,size=10).reshape(-1,1)\n",
    "    sample_y = 3*sample_x.flatten()+np.random.normal(0,1,size=sample_x.shape[0])\n",
    "    g_hat = linear_regression(sample_x,sample_y)\n",
    "\n",
    "    # Lets plot out function\n",
    "    plt.scatter(sample_x,sample_y)\n",
    "    x_plot = np.linspace(0,1,10)\n",
    "    plt.plot(x_plot,g_hat(x_plot.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(1000):\n",
    "    sample_x = np.random.uniform(0,1,size=10).reshape(-1,1)\n",
    "    sample_y = 3*sample_x.flatten()+np.random.normal(0,1,size=sample_x.shape[0])\n",
    "    g_hat = linear_regression(sample_x,sample_y)\n",
    "\n",
    "    # Lets plot out function\n",
    "    #plt.scatter(sample_x,sample_y,alpha=0.1,color='blue')\n",
    "    x_plot = np.linspace(0,1,10)\n",
    "    plt.plot(x_plot,g_hat(x_plot.reshape(-1,1)),alpha=0.01,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1782af",
   "metadata": {},
   "source": [
    "## Example, testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d4b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(n_samples):\n",
    "    sample_x = np.random.uniform(0,1,size=n_samples).reshape(-1,1)\n",
    "    sample_y = 3*sample_x.flatten()+np.random.normal(0,1,size=sample_x.shape[0])\n",
    "    return sample_x,sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb446c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,ytrain = gen_data(10)\n",
    "g_hat = linear_regression(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error():\n",
    "    xtest,ytest = gen_data(100)\n",
    "    predictions = g_hat(xtest)\n",
    "    residual = ytest-predictions\n",
    "    return np.mean(residual**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([test_error() for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb8f31",
   "metadata": {},
   "source": [
    "# Another example with testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "# 150 samples, 4 features\n",
    "\n",
    "X_train = X[:100,:] # First 100 samples as training data\n",
    "Y_train = Y[:100]\n",
    "X_test = X[100:,:]  # Remaining samples as test data\n",
    "Y_test = Y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074972c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need a learning machine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,Y_train) # This minimizes an empirical risk over the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c71227",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_hat = lambda x: lr.predict(x) # Our learned function\n",
    "\n",
    "# At this point in time the training data is given and as such g_hat is considered fixed and not random\n",
    "\n",
    "# We can now evaluate the performance of g_hat on the test data\n",
    "\n",
    "predictions = g_hat(X_test)\n",
    "loss01 = np.mean(predictions != Y_test) # 0-1 loss\n",
    "loss01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34976e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd3976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a7308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee51a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687147f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dab95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62d0b647",
   "metadata": {},
   "source": [
    "## The Problem with Ordered Data: Violating the IID Assumption\n",
    "\n",
    "The previous example demonstrates a **critical mistake**: using ordered data without randomization, violating the **IID assumption** (Independent and Identically Distributed).\n",
    "\n",
    "### What Went Wrong?\n",
    "The Iris dataset is ordered by species (0-49: *Setosa*, 50-99: *Versicolor*, 100-149: *Virginica*).\n",
    "\n",
    "Our naive split:\n",
    "- **Training**: First 100 samples (only *Setosa* and *Versicolor*)\n",
    "- **Test**: Last 50 samples (only *Virginica*)\n",
    "\n",
    "### Why This Is Problematic:\n",
    "- **Distribution mismatch**: Training and test sets have different class distributions\n",
    "- **Impossible task**: Model never sees *Virginica* during training\n",
    "- **Misleading results**: High test error doesn't reflect real performance\n",
    "\n",
    "### The Solution:\n",
    "**Randomly shuffle the data** before splitting to maintain the IID assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 2\n",
    "# We begin by shuffling the data\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_shuffled = X[indices,:]\n",
    "Y_shuffled = Y[indices]\n",
    "X_train = X_shuffled[:100,:] # First 100 samples as training data\n",
    "Y_train = Y_shuffled[:100]\n",
    "X_test = X_shuffled[100:,:]  # Remaining samples as test data\n",
    "Y_test = Y_shuffled[100:]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,Y_train) # This minimizes an empirical risk over the training data\n",
    "g_hat = lambda x: lr.predict(x) # Our learned function\n",
    "predictions = g_hat(X_test)\n",
    "loss01 = np.mean(predictions != Y_test) # 0-1 loss\n",
    "loss01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ae184",
   "metadata": {},
   "source": [
    "# Comprehensive example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0ac98",
   "metadata": {},
   "source": [
    "## Iris Dataset: A Complete Machine Learning Example\n",
    "\n",
    "The Iris dataset contains measurements of 150 iris flowers from three species: *Setosa*, *Versicolor*, and *Virginica*. Each flower has four features: sepal length/width and petal length/width.\n",
    "\n",
    "This example demonstrates a complete machine learning workflow:\n",
    "\n",
    "### Part 1: Data Exploration\n",
    "- Load dataset and examine basic statistics\n",
    "- Visualize feature relationships with scatter plots\n",
    "- Analyze correlations between variables\n",
    "\n",
    "### Part 2: Classification with Logistic Regression\n",
    "- Split data into training/test sets with proper randomization\n",
    "- Scale features for optimal performance\n",
    "- Train logistic regression classifier\n",
    "- Evaluate using accuracy, confusion matrix, and feature importance\n",
    "\n",
    "**Expected Results**: High accuracy (>95%) with petal features being most important for classification.\n",
    "\n",
    "This demonstrates **empirical risk minimization**: choose a model, define a loss function, minimize risk on training data, and evaluate on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac28816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Iris Dataset Example\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"Iris Dataset Overview:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Target classes: {iris.target_names}\")\n",
    "print()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(df.describe())\n",
    "print()\n",
    "\n",
    "# Class distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(df['species'].value_counts())\n",
    "print()\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plots to show relationships between features\n",
    "plt.subplot(1, 2, 1)\n",
    "for species in df['species'].unique():\n",
    "    species_data = df[df['species'] == species]\n",
    "    plt.scatter(species_data['sepal length (cm)'], species_data['sepal width (cm)'], \n",
    "                label=species, alpha=0.7)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.legend()\n",
    "plt.title('Sepal Length vs Width')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for species in df['species'].unique():\n",
    "    species_data = df[df['species'] == species]\n",
    "    plt.scatter(species_data['petal length (cm)'], species_data['petal width (cm)'], \n",
    "                label=species, alpha=0.7)\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.legend()\n",
    "plt.title('Petal Length vs Width')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "print(\"Feature Correlation Matrix:\")\n",
    "correlation_matrix = df[iris.feature_names].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Example on Iris Dataset - Logistic Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare the data\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "print(\"Logistic Regression Classification on Iris Dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print()\n",
    "\n",
    "# Scale the features (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train logistic regression classifier\n",
    "clf = LogisticRegression(random_state=42, max_iter=200)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Visualize confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show model coefficients (feature weights)\n",
    "print(\"Model Coefficients (Feature Weights):\")\n",
    "for i, feature in enumerate(iris.feature_names):\n",
    "    print(f\"{feature}: {clf.coef_[0][i]:.4f}\")\n",
    "\n",
    "# Visualize feature coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "coefficients = clf.coef_[0]\n",
    "plt.bar(iris.feature_names, coefficients, color='lightcoral')\n",
    "plt.title('Logistic Regression Feature Coefficients')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITDS2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "lx_course_instance": "2025",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
